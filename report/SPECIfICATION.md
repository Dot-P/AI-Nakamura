以下は、 `MountainCar-v0` のDQNコードをベースに、レポートに必要な構成を**具体的な内容で埋めた**ものです。

レポートの作成の際にコピペして使うのは禁止します。多少は表現を変えてください。(内容が全く一緒になってしまうため)
また、ChatGPTによる生成のため、内容は各自で確認するようにしてください。

以下の考察は一種の例であり、必ずしもこれをしなくてはいけないということではありません。
必要に応じて内容を修正してください。

---

# 📝 レポート：DQNによるMountainCar-v0タスクの学習と考察

## 1. 目的・目標

本課題の目的は、Gymnasium（旧OpenAI Gym）に含まれる環境の中から「MountainCar-v0」を選択し、この環境を自律的に攻略するAIエージェントを実装・学習させることである。本タスクに対して、Deep Q-Network（DQN）を用いた強化学習モデルを構築し、学習条件がタスク遂行能力に与える影響について考察する。

---

## 2. 対象タスクの詳細

### タスク名：

`MountainCar-v0`

### 内容：

谷底にいる車を左右に動かしながら加速させ、右側の山頂に到達することが目的。重力を超える推進力がないため、反復的に左右に動かしながらエネルギーを蓄積し、最終的に山頂を越える必要がある。

### 環境の仕様：

* **状態空間（obs）**：2次元連続値（位置, 速度）

  * `position ∈ [-1.2, 0.6]`
  * `velocity ∈ [-0.07, 0.07]`
* **行動空間**：3つの離散行動

  * `0`: 左に加速
  * `1`: 何もしない
  * `2`: 右に加速
* **報酬**：

  * 1ステップあたり `-1` の報酬
  * エピソードは最大200ステップ
  * 終了条件：`position >= 0.5` に到達した時

---

## 3. 作成したAIの詳細

### 3.1 使用手法：Deep Q-Network（DQN）

### 3.2 ネットワーク構造

| 層番号 | 層の種類        | ユニット数         | 活性化関数 |
| --- | ----------- | ------------- | ----- |
| 1   | Linear      | 32 / 64 / 128 | ReLU  |
| 2   | Linear      | 32 / 64       | ReLU  |
| 3   | Linear (出力) | 3（行動数）        | なし    |

※ 複数の隠れ層構造（例：`[32, 32]`, `[64, 64]`, `[128, 64]`）を比較実験として使用。

### 3.3 学習設定

| ハイパーパラメータ           | 値（例）                |
| ------------------- | ------------------- |
| 割引率（γ）              | 0.95, 0.99          |
| 学習率（lr）             | 1e-3, 5e-4          |
| バッチサイズ              | 32, 64              |
| Replay Buffer サイズ   | 1e5, 2e5            |
| ε-greedy（初期値）       | 1.0                 |
| ε-greedy（最小値）       | 0.01                |
| ε減衰係数（指数減衰）         | 500.0               |
| target network 同期頻度 | 10エピソードごと           |
| エピソード最大ステップ数        | 200                 |
| 終了条件                | 山頂位置 ≥ 0.5 に10回連続到達 |

### 実装の特徴：

* 経験再生（Replay Memory）を使用
* target Q-network を10エピソードごとに同期
* 動画記録および学習ログ（CSV）出力あり
* 学習完了後に `.pt` ファイルとしてモデル保存

---

## 4. 結果および考察

### 4.1 評価指標と結果

* **評価指標**：各エピソードでの累積報酬、山頂到達までのステップ数
* **学習ログ**：CSVファイルに記録
* **学習の可視化**：以下のような傾向が見られた

| 条件                                | 到達エピソード数 | 平均報酬（直前10エピ） |
| --------------------------------- | -------- | ------------ |
| γ=0.95, lr=1e-3, hidden=\[64,64]  | 約180     | -140 ～ -110  |
| γ=0.99, lr=5e-4, hidden=\[128,64] | 約100     | -130 ～ -90   |

> 🎯 **成功判定**：10エピソード連続で山頂到達

### 4.2 学習条件の違いによる影響

* **γ（割引率）**

  * γ=0.99 の方が長期的報酬を重視しやすく、収束が早くなる傾向あり
* **Replay Memory サイズ**

  * 小さいサイズでは古い経験がすぐに消えるため、学習が不安定になりやすい
* **εの減衰方法**

  * 緩やかな指数減衰は、序盤の探索を維持しつつも安定した収束を実現できた
* **ネットワークサイズ**

  * 大きすぎるネットワーク（128-64）はパラメータが多く、低エピソード数では過学習傾向も見られた

### 考察：

* **経験再生とtarget networkの導入**により、収束の安定性が大幅に向上した
* **報酬が常に負のため**、エージェントはできるだけ早くゴールに到達するような戦略を学習した
* ネットワークサイズは「複雑すぎず、単純すぎず」が良好

---

## 5. まとめ

本実験を通じて、DQNを用いた強化学習エージェントが `MountainCar-v0` のような連続状態・離散行動のタスクに対して有効に機能することを確認した。Replay Memoryやtarget network、εの制御といった工夫が、安定した学習に寄与することがわかった。

また、学習条件（ハイパーパラメータ）の調整が、性能に大きな影響を与えることを実感した。今後は、Prioritized Experience Replay や Double DQN などの拡張も検討すべきである。

---

## 6. 実行方法

### 学習の実行：

```bash
uv run train.py
```

### 動画の記録（例）：

```bash
python record_video.py \
  --model_path models/g0.99_lr0.001_h64-64_b64_m200000_s0/policy_net.pt \
  --hidden_sizes 64 64 \
  --video_dir demo_videos/g0.99_lr0.001_h64-64 \
  --device cpu
```

---

## 7. 補足情報

* 学習ログ：`logs/` にCSV形式で保存
* 学習済モデル：`models/` に保存
* 動画出力：`demo_videos/` 以下にMP4形式で保存
* 動画録画には `ffmpeg` が必要：`sudo apt install ffmpeg`

---

