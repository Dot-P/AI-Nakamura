{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uc_YnRbDX6u3"
   },
   "source": [
    "##### 作業用ディレクトリの準備\n",
    "- ローカルのWindows環境で実行する場合は，Git for Windowsを事前にインストールしておく必要があります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNWKLfthX6u5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# 独自ライブラリ等のダウンロード\n",
    "if not os.path.isdir('AI_advanced'):\n",
    "    !git clone https://github.com/knakamura1982/AI_advanced.git\n",
    "%cd AI_advanced\n",
    "\n",
    "# モデルファイルの保存先ディレクトリの作成\n",
    "if not os.path.isdir('CGAN_models'):\n",
    "    !mkdir CGAN_models\n",
    "\n",
    "# 一時ファイルの保存先ディレクトリの作成\n",
    "if os.path.exists('temp'):\n",
    "    if os.name == 'nt':\n",
    "        !Powershell.exe -Command \"rm -r -fo temp\"\n",
    "    else:\n",
    "        !rm -fr temp\n",
    "!mkdir temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9QpIL1KX6u7"
   },
   "source": [
    "##### 顔画像データセットCelebAのダウンロード・解凍\n",
    "- **前回の試行の続きを行いたい場合（再開モードの場合）でも実行が必要です．**\n",
    "- これは本来のCelebAではなく，その中から10%弱の画像をランダムに抜き出した簡易版です．\n",
    "- 十数分かかる可能性があります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2agpN1oFX6u7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from mylib.data_io import CSVBasedDataset\n",
    "\n",
    "\n",
    "if not os.path.isfile('./Datasets/tinyCelebA_train_images.pt'):\n",
    "    if os.name == 'nt':\n",
    "        # ローカルのWindows環境の場合\n",
    "        !Powershell.exe -Command \"wget https://tus.box.com/shared/static/z7a4pb9qtco6fwspige2tpt2ryhqv9l1.gz -O tinyCelebA.tar.gz\"\n",
    "        !Powershell.exe -Command \"tar -zxf tinyCelebA.tar.gz\"\n",
    "        !Powershell.exe -Command \"rm -fo tinyCelebA.tar.gz\"\n",
    "    else:\n",
    "        # それ以外（Colab環境含む）の場合\n",
    "        !wget \"https://tus.box.com/shared/static/z7a4pb9qtco6fwspige2tpt2ryhqv9l1.gz\" -O tinyCelebA.tar.gz\n",
    "        !tar -zxf tinyCelebA.tar.gz\n",
    "        !rm -f tinyCelebA.tar.gz\n",
    "    dataset = CSVBasedDataset(\n",
    "        dirname='./tinyCelebA',\n",
    "        filename='./tinyCelebA/image_list.csv',\n",
    "        items=[\n",
    "            'File Path',\n",
    "            [\n",
    "                '5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair', 'Blurry',\n",
    "                'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open',\n",
    "                'Mustache', 'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair',\n",
    "                'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace', 'Wearing_Necktie', 'Young'\n",
    "            ]\n",
    "        ],\n",
    "        dtypes=['image', 'float'],\n",
    "        img_transform=transforms.CenterCrop((128, 128))\n",
    "    )\n",
    "    data = [dataset[i] for i in range(len(dataset))]\n",
    "    image_tensor = torch.cat([torch.unsqueeze(u, dim=0) for u, v in data], dim=0)\n",
    "    label_tensor = torch.cat([torch.unsqueeze(v, dim=0) for u, v in data], dim=0)\n",
    "    torch.save(image_tensor, './Datasets/tinyCelebA_train_images.pt')\n",
    "    torch.save(label_tensor, './Datasets/tinyCelebA_train_labels.pt')\n",
    "    del dataset, data, image_tensor, label_tensor\n",
    "    if os.name == 'nt':\n",
    "        !Powershell.exe -Command \"rm -r -fo tinyCelebA\"\n",
    "    else:\n",
    "        !rm -fr tinyCelebA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DqiP8r20NPr"
   },
   "source": [
    "##### 必要なら，自分のgoogle driveに退避させておいた過去の学習経過情報をロード\n",
    "- Google Colab以外の環境で使用することは想定していません．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwDvxMVU0Q4s"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "\n",
    "# 自分のgoogle driveにおける退避先フォルダの名称\n",
    "DST_DIR_NAME = 'AI_advanced_temp'\n",
    "\n",
    "# 退避先フォルダの中身をtempフォルダにまるごとコピー\n",
    "MOUNT_POINT = '/content/drive'\n",
    "DST_DIR_PATH = os.path.join(MOUNT_POINT, 'MyDrive', DST_DIR_NAME)\n",
    "if not os.path.isdir(MOUNT_POINT):\n",
    "    drive.mount(MOUNT_POINT)\n",
    "!cp -r $DST_DIR_PATH/* temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qxjs0jGhX6u9"
   },
   "source": [
    "##### データセットの場所やバッチサイズなどの定数値の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39AoqgDEX6u9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "import torch\n",
    "\n",
    "\n",
    "# 前回の試行の続きを行いたい場合は True にする -> 再開モードになる．\n",
    "# なお，Colab環境で再開モードを利用する場合は，前回終了時に temp ディレクトリの中身を自分の Google Drive に退避しておき，\n",
    "# それを改めて /content/AI_advanced/temp 以下にあらかじめ移しておく必要がある．\n",
    "RESTART_MODE = True\n",
    "\n",
    "\n",
    "# 使用するデバイス\n",
    "# GPU を使用しない環境（CPU環境）で実行する場合は DEVICE = 'cpu' とする．\n",
    "# GPU が複数存在する環境では，'cuda:0', 'cuda:1', 'cuda:2' などのような形で使用するGPUのIDを指定する．\n",
    "# Google Colab, Paperspace Gradient などで GPU を利用する場合は DEVICE = 'cuda:0' とすれば良いはず．\n",
    "DEVICE = 'cuda:0'\n",
    "\n",
    "# 高速化・省メモリ化のために半精度小数を用いた混合精度学習を行うか否か（Trueの場合は行う）\n",
    "USE_AMP = True\n",
    "FLOAT_DTYPE = torch.float16 # 混合精度学習を行う場合の半精度小数の型．環境によっては torch.bfloat16 にした方が良好な性能になる（ただしColabのT4 GPU環境ではムリ）．\n",
    "\n",
    "# 混合精度学習の設定\n",
    "if DEVICE == 'cpu':\n",
    "    USE_AMP = False # CPU使用時は強制的に混合精度学習をOFFにする\n",
    "LOSS_SCALER = torch.amp.grad_scaler.GradScaler(enabled=USE_AMP, device='cuda', init_scale=2**12)\n",
    "ADAM_EPS = 1e-4 if USE_AMP and (FLOAT_DTYPE == torch.float16) else 1e-8\n",
    "\n",
    "# 全ての訓練データを一回ずつ使用することを「1エポック」として，何エポック分学習するか\n",
    "N_EPOCHS = 100\n",
    "\n",
    "# 学習時のバッチサイズ\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# データセットの存在するフォルダ・ファイル名\n",
    "DATA_DIR = './Datasets/'\n",
    "TRAIN_IMAGES_FILE = 'tinyCelebA_train_images.pt'\n",
    "TRAIN_LABELS_FILE = 'tinyCelebA_train_labels.pt'\n",
    "\n",
    "# tinyCelebAにおける属性ラベルの名称と番号の対応表\n",
    "ATTRIBUTE_TABLE = {\n",
    "    '5_o_Clock_Shadow'  :  0, 'Arched_Eyebrows'     :  1, 'Attractive'       :  2, 'Bags_Under_Eyes' :  3,\n",
    "    'Bald'              :  4, 'Bangs'               :  5, 'Big_Lips'         :  6, 'Big_Nose'        :  7,\n",
    "    'Black_Hair'        :  8, 'Blond_Hair'          :  9, 'Blurry'           : 10, 'Brown_Hair'      : 11,\n",
    "    'Bushy_Eyebrows'    : 12, 'Chubby'              : 13, 'Double_Chin'      : 14, 'Eyeglasses'      : 15,\n",
    "    'Goatee'            : 16, 'Gray_Hair'           : 17, 'Heavy_Makeup'     : 18, 'High_Cheekbones' : 19,\n",
    "    'Male'              : 20, 'Mouth_Slightly_Open' : 21, 'Mustache'         : 22, 'Narrow_Eyes'     : 23,\n",
    "    'No_Beard'          : 24, 'Oval_Face'           : 25, 'Pale_Skin'        : 26, 'Pointy_Nose'     : 27,\n",
    "    'Receding_Hairline' : 28, 'Rosy_Cheeks'         : 29, 'Sideburns'        : 30, 'Smiling'         : 31,\n",
    "    'Straight_Hair'     : 32, 'Wavy_Hair'           : 33, 'Wearing_Earrings' : 34, 'Wearing_Hat'     : 35,\n",
    "    'Wearing_Lipstick'  : 36, 'Wearing_Necklace'    : 37, 'Wearing_Necktie'  : 38, 'Young'           : 39\n",
    "}\n",
    "\n",
    "# 取り扱う属性ラベル（上表の中から名称で指定）\n",
    "TARGET_ATTRIBUTES = ['Blond_Hair', 'Brown_Hair', 'Black_Hair', 'Gray_Hair', 'Eyeglasses', 'Male', 'Young']\n",
    "\n",
    "# 取り扱う属性ラベルの番号\n",
    "TARGET_ATTRIBUTES_ID = [ATTRIBUTE_TABLE[a] for a in TARGET_ATTRIBUTES]\n",
    "\n",
    "# 画像サイズ\n",
    "H = 128 # 縦幅\n",
    "W = 128 # 横幅\n",
    "C = 3 # チャンネル数（カラー画像なら3，グレースケール画像なら1）\n",
    "\n",
    "# 特徴ベクトルの次元数\n",
    "N = 128\n",
    "\n",
    "# 学習結果の保存先フォルダ\n",
    "MODEL_DIR = './CGAN_models/'\n",
    "\n",
    "# 学習結果のニューラルネットワークの保存先\n",
    "MODEL_FILE_G = os.path.join(MODEL_DIR, './face_generator_model.pth') # ジェネレータ\n",
    "MODEL_FILE_D = os.path.join(MODEL_DIR, './face_discriminator_model.pth') # ディスクリミネータ\n",
    "\n",
    "# 中断／再開の際に用いる一時ファイル\n",
    "CHECKPOINT_EPOCH = os.path.join('./temp/', 'checkpoint_epoch.pkl')\n",
    "CHECKPOINT_GEN_MODEL = os.path.join('./temp/', 'checkpoint_gen_model.pth')\n",
    "CHECKPOINT_DIS_MODEL = os.path.join('./temp/', 'checkpoint_dis_model.pth')\n",
    "CHECKPOINT_GEN_OPT = os.path.join('./temp/', 'checkpoint_gen_opt.pth')\n",
    "CHECKPOINT_DIS_OPT = os.path.join('./temp/', 'checkpoint_dis_opt.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3v5wkXyX6u-"
   },
   "source": [
    "##### ニューラルネットワークモデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8g4JqyNFX6u-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from mylib.basic_layers import Reshape, MinibatchDiscrimination, DiscriminatorAugmentation\n",
    "\n",
    "\n",
    "# Pre-act Residual Block\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, sn=False):\n",
    "        super(ResBlock, self).__init__()\n",
    "        shortcut_conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        main_conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        main_conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        if sn:\n",
    "            # spectral normalization を用いる場合（主にディスクリミネータ用）\n",
    "            self.shortcut = nn.utils.spectral_norm(shortcut_conv)\n",
    "            self.block1 = nn.Sequential(nn.ReLU(), nn.utils.spectral_norm(main_conv1))\n",
    "            self.block2 = nn.Sequential(nn.ReLU(), nn.utils.spectral_norm(main_conv2))\n",
    "        else:\n",
    "            # バッチ正規化を用いる場合（主にジェネレータ用）\n",
    "            self.shortcut = shortcut_conv\n",
    "            self.block1 = nn.Sequential(nn.BatchNorm2d(num_features=in_channels), nn.ReLU(), main_conv1)\n",
    "            self.block2 = nn.Sequential(nn.BatchNorm2d(num_features=out_channels), nn.ReLU(), main_conv2)\n",
    "    def forward(self, x):\n",
    "        s = self.shortcut(x)\n",
    "        h = self.block1(x)\n",
    "        h = self.block2(h)\n",
    "        return h + s\n",
    "\n",
    "\n",
    "# GANジェネレータ用のアップサンプリング層\n",
    "class myUpsamplingBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(myUpsamplingBlock, self).__init__()\n",
    "        self.up = nn.UpsamplingNearest2d(scale_factor=2)\n",
    "        self.rb = ResBlock(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1, sn=False)\n",
    "    def forward(self, x):\n",
    "        h = self.up(x)\n",
    "        return self.rb(h)\n",
    "\n",
    "\n",
    "# GANディスクリミネータ用のダウンサンプリング層\n",
    "class myDownsamplingBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(myDownsamplingBlock, self).__init__()\n",
    "        self.rb = ResBlock(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1, sn=True)\n",
    "        self.down = nn.AvgPool2d(kernel_size=2)\n",
    "    def forward(self, x):\n",
    "        h = self.rb(x)\n",
    "        return self.down(h)\n",
    "\n",
    "\n",
    "# 顔画像生成ニューラルネットワーク\n",
    "# CGAN生成器（ジェネレータ）のサンプル\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    # C: 出力顔画像のチャンネル数（1または3と仮定）\n",
    "    # H: 出力顔画像の縦幅（32の倍数と仮定）\n",
    "    # W: 出力顔画像の横幅（32の倍数と仮定）\n",
    "    # N: 入力の特徴ベクトル（乱数ベクトル）の次元数\n",
    "    # K: 属性ラベルの種類数\n",
    "    def __init__(self, C, H, W, N, K):\n",
    "        super(Generator, self).__init__()\n",
    "        self.W = W\n",
    "        self.H = H\n",
    "\n",
    "        # 属性ラベル情報を処理する全結合層\n",
    "        self.embed = nn.Linear(in_features=K, out_features=N) # 属性ラベル情報を特徴ベクトルと同じ N 次元に拡張\n",
    "\n",
    "        # 属性ラベル情報と特徴ベクトルを連結した後のベクトルをチャンネル数 512, 縦幅 H/32, 横幅 W/32 の特徴マップに変換する層\n",
    "        self.conv0 = nn.Sequential(\n",
    "            Reshape(size=(2*N, 1, 1)),\n",
    "            nn.ConvTranspose2d(in_channels=2*N, out_channels=512, kernel_size=(H//32, W//32), stride=1, padding=0),\n",
    "        )\n",
    "\n",
    "        # アップサンプリング層1～5\n",
    "        # これらを通すことにより特徴マップの縦幅・横幅がそれぞれ 2 倍になる\n",
    "        # 5つ通すことになるので，最終的には都合 32 倍になる -> ゆえに縦幅 H/32, 横幅 W/32 の特徴マップからスタートする\n",
    "        self.up1 = myUpsamplingBlock(in_channels=512, out_channels=256)\n",
    "        self.up2 = myUpsamplingBlock(in_channels=256, out_channels=128)\n",
    "        self.up3 = myUpsamplingBlock(in_channels=128, out_channels=64)\n",
    "        self.up4 = myUpsamplingBlock(in_channels=64, out_channels=32)\n",
    "        self.up5 = myUpsamplingBlock(in_channels=32, out_channels=32)\n",
    "\n",
    "        # 出力画像生成用の畳込み層\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.BatchNorm2d(num_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=C, kernel_size=1, stride=1, padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        y = self.embed(y) # 属性ラベル情報を N 次元に\n",
    "        h = torch.cat((z, y), dim=1) # 特徴ベクトルと属性ラベル情報を連結 -> トータル256次元に\n",
    "        h = self.conv0(h) # 256次元の特徴ベクトルをチャンネル数 512, 縦幅 H/32, 横幅 W/32 の特徴マップに変換\n",
    "        h = self.up1(h)\n",
    "        h = self.up2(h)\n",
    "        h = self.up3(h)\n",
    "        h = self.up4(h)\n",
    "        h = self.up5(h)\n",
    "        y = torch.tanh(self.conv5(h))\n",
    "        return y\n",
    "\n",
    "\n",
    "# 顔画像が Real か Fake を判定するニューラルネットワーク\n",
    "# CGAN識別器（ディスクリミネータ）のサンプル\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    # C: 入力顔画像のチャンネル数（1または3と仮定）\n",
    "    # H: 入力顔画像の縦幅（32の倍数と仮定）\n",
    "    # W: 入力顔画像の横幅（32の倍数と仮定）\n",
    "    # K: 属性ラベルの種類数\n",
    "    def __init__(self, C, H, W, K):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # 訓練データ量の不足を補うためのデータ拡張（Data Augmentation）処理\n",
    "        self.preprocess = DiscriminatorAugmentation(H, W, p_hflip=0.5, p_vflip=0.4, p_rot=0.4) # 確率0.5で左右反転，確率0.4で上下反転，確率0.4で回転\n",
    "\n",
    "        # ダウンサンプリング層1～5\n",
    "        # カーネルサイズ4，ストライド幅2，パディング1の設定なので，これらを通すことにより特徴マップの縦幅・横幅がそれぞれ 1/2 になる\n",
    "        self.down1 = myDownsamplingBlock(in_channels=C+K, out_channels=32)\n",
    "        self.down2 = myDownsamplingBlock(in_channels=32, out_channels=64)\n",
    "        self.down3 = myDownsamplingBlock(in_channels=64, out_channels=128)\n",
    "        self.down4 = myDownsamplingBlock(in_channels=128, out_channels=256)\n",
    "        self.down5 = myDownsamplingBlock(in_channels=256, out_channels=256)\n",
    "\n",
    "        # 平坦化\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "        # 全結合層1（spectral normalization を使用）\n",
    "        # ダウンサンプリング層1～5を通すことにより特徴マップの縦幅・横幅は都合 1/32 になっているので，\n",
    "        # 入力側のパーセプトロン数は 256*(H/32)*(W/32) = H*W/4\n",
    "        self.fc1 = nn.utils.spectral_norm(nn.Linear(in_features=H*W//4, out_features=256))\n",
    "\n",
    "        # 全結合層2\n",
    "        self.fc2 = nn.Linear(in_features=384, out_features=1)\n",
    "\n",
    "        # Minibatch Discrimination: モード崩壊を回避するための技法の一つ\n",
    "        self.md = MinibatchDiscrimination(in_features=256, out_features=128)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # 本来であれば，ディスクリミネータの出力が 0～1 の範囲となるよう，最終層の活性化関数として sigmoid を適用すべきであるが，\n",
    "        # このサンプルコードでは損失関数側で sigmoid 適用することになるので, ここでは最終層で活性化関数を適用しない\n",
    "        y = y.reshape(*y.size(), 1, 1).repeat(1, 1, x.size()[2], x.size()[3]) # 属性ラベル情報を画像と同じ形に拡張\n",
    "        x = self.preprocess(x)\n",
    "        h = torch.cat((x, y), dim=1) # 画像情報とラベル情報を結合\n",
    "        h = self.down1(h)\n",
    "        h = self.down2(h)\n",
    "        h = self.down3(h)\n",
    "        h = self.down4(h)\n",
    "        h = self.down5(h)\n",
    "        h = self.flat(h)\n",
    "        h = F.relu(self.fc1(h))\n",
    "        h = self.md(h) # Minibatch Discrimination\n",
    "        z = self.fc2(h) # 上記の通り，最終層では活性化関数なし\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZKSx4InX6vA"
   },
   "source": [
    "##### 訓練データセットの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRb4iUCAX6vB"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from mylib.data_io import TensorDataset\n",
    "\n",
    "\n",
    "# テンソルファイルを読み込み, 訓練データセットを用意\n",
    "# 今回は，全てのデータを学習用に回す\n",
    "image_tensor = torch.load(os.path.join(DATA_DIR, TRAIN_IMAGES_FILE), weights_only=True)\n",
    "label_tensor = torch.load(os.path.join(DATA_DIR, TRAIN_LABELS_FILE), weights_only=True)\n",
    "label_tensor = torch.cat([label_tensor[:, TARGET_ATTRIBUTES_ID[i]:TARGET_ATTRIBUTES_ID[i]+1] for i in range(len(TARGET_ATTRIBUTES_ID))], dim=1)\n",
    "train_dataset = TensorDataset(tensors=[image_tensor, label_tensor])\n",
    "train_size = len(train_dataset)\n",
    "\n",
    "# 訓練データをミニバッチに分けて使用するための「データローダ」を用意\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZERK3SYX6vB"
   },
   "source": [
    "##### 学習処理の実行\n",
    "- CGANの学習は一般に安定せず，最終的なモデルよりも学習途中のモデルの方が優れていることがよくあります\n",
    "- このため，エポックごとにモデル保存処理を実行し，学習終了後，最良（と思われる）モデルをロードして利用することも多いです\n",
    "- ただし，これを Google Colab などのクラウド環境で実行するとストレージ使用量の上限を超えてしまう可能性があるので，注意してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JWWPNKjRX6vC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from mylib.loss_functions import GANLoss\n",
    "from mylib.visualizers import LossVisualizer\n",
    "from mylib.data_io import show_images, to_sigmoid_image, to_tanh_image, autosaved_model_name\n",
    "from mylib.utility import save_checkpoint, load_checkpoint\n",
    "\n",
    "\n",
    "# 何エポックに1回の割合で学習経過を表示するか（モデル保存処理もこれと同じ頻度で実行）\n",
    "INTERVAL_FOR_SHOWING_PROGRESS = 10\n",
    "\n",
    "# spectral normalization の使用によりディスクリミネータが弱体化するので，ジェネレータの更新回数を減らすことが望ましいらしいが，実際にはなんとも言い難い\n",
    "# ここでは，ジェネレータを5回に1回の割合で更新することにする\n",
    "N_DIS = 5 # この値を 1 にすれば，ジェネレータも毎回更新されるようになる\n",
    "\n",
    "\n",
    "# エポック番号\n",
    "INIT_EPOCH = 0 # 初期値\n",
    "LAST_EPOCH = INIT_EPOCH + N_EPOCHS # 最終値\n",
    "\n",
    "# ニューラルネットワークの作成\n",
    "gen_model = Generator(C=C, H=H, W=W, N=N, K=len(TARGET_ATTRIBUTES)).to(DEVICE)\n",
    "dis_model = Discriminator(C=C, H=H, W=W, K=len(TARGET_ATTRIBUTES)).to(DEVICE)\n",
    "\n",
    "# 最適化アルゴリズムの指定（ここでは SGD でなく Adam を使用）\n",
    "gen_optimizer = optim.Adam(gen_model.parameters(), lr=0.0002, betas=(0.5, 0.999), eps=ADAM_EPS)\n",
    "dis_optimizer = optim.Adam(dis_model.parameters(), lr=0.0002, betas=(0.5, 0.999), eps=ADAM_EPS)\n",
    "\n",
    "# 再開モードの場合は，前回チェックポイントから情報をロードして学習再開\n",
    "if RESTART_MODE:\n",
    "    INIT_EPOCH, LAST_EPOCH, gen_model, gen_optimizer = load_checkpoint(CHECKPOINT_EPOCH, CHECKPOINT_GEN_MODEL, CHECKPOINT_GEN_OPT, N_EPOCHS, gen_model, gen_optimizer)\n",
    "    _, _, dis_model, dis_optimizer = load_checkpoint(CHECKPOINT_EPOCH, CHECKPOINT_DIS_MODEL, CHECKPOINT_DIS_OPT, N_EPOCHS, dis_model, dis_optimizer)\n",
    "    print('')\n",
    "\n",
    "# 損失関数\n",
    "loss_func = GANLoss(label_smoothing=True)\n",
    "\n",
    "# 検証の際に使用する乱数ベクトルおよび属性ラベル情報を用意\n",
    "Z_valid = torch.randn(BATCH_SIZE, N).to(DEVICE) # 検証用乱数ベクトル\n",
    "L_valid = torch.zeros(BATCH_SIZE, len(TARGET_ATTRIBUTES)).to(DEVICE) # 検証用属性ラベル情報\n",
    "\n",
    "# 損失関数値を記録する準備\n",
    "loss_viz = LossVisualizer(['G loss', 'D loss'], init_epoch=INIT_EPOCH)\n",
    "\n",
    "# 勾配降下法による繰り返し学習\n",
    "for epoch in range(INIT_EPOCH, LAST_EPOCH):\n",
    "\n",
    "    print('Epoch {0}:'.format(epoch + 1))\n",
    "\n",
    "    # 学習\n",
    "    gen_model.train()\n",
    "    dis_model.train()\n",
    "    sum_gen_loss = 0\n",
    "    sum_dis_loss = 0\n",
    "    n_iter = 1 # 1エポック内でのループ回数を記録する変数（ジェネレータの更新回数を制御するために使用）\n",
    "    for X, L in tqdm(train_dataloader):\n",
    "        for param in gen_model.parameters():\n",
    "            param.grad = None\n",
    "        for param in dis_model.parameters():\n",
    "            param.grad = None\n",
    "        L = L.to(DEVICE) # 属性ラベル情報\n",
    "        Z = torch.randn(len(X), N).to(DEVICE) # 乱数ベクトルを用意\n",
    "        real = to_tanh_image(X).to(DEVICE) # Real画像を用意（to_tanh_image 関数を用い，画素値の範囲が -1〜1 となるように調整しておく）\n",
    "        with torch.amp.autocast_mode.autocast(enabled=USE_AMP, device_type='cuda', dtype=FLOAT_DTYPE):\n",
    "            fake = gen_model(Z, L) # Fake画像を生成（2行上で用意した Z から生成）\n",
    "            fake_cpy = fake.detach() # Fake画像のコピーを用意しておく\n",
    "            ### ジェネレータの学習 ###\n",
    "            if n_iter % N_DIS == 0:\n",
    "                Y_fake = dis_model(fake, L) # Fake画像を識別\n",
    "                gen_loss = loss_func.G_loss(Y_fake)\n",
    "                LOSS_SCALER.scale(gen_loss).backward()\n",
    "                LOSS_SCALER.step(gen_optimizer)\n",
    "                LOSS_SCALER.update()\n",
    "                sum_gen_loss += float(gen_loss) * len(X)\n",
    "            ### ディスクリミネータの学習 ###\n",
    "            for param in dis_model.parameters():\n",
    "                param.grad = None # ジェネレータの学習時の計算した勾配を一旦リセット\n",
    "            Y_real = dis_model(real, L) # Real画像を識別\n",
    "            Y_fake = dis_model(fake_cpy, L) # Fake画像を識別（コピー変数の方を使用）\n",
    "            dis_loss = loss_func.D_loss(Y_fake, as_real=False) + loss_func.D_loss(Y_real, as_real=True)\n",
    "            LOSS_SCALER.scale(dis_loss).backward()\n",
    "            LOSS_SCALER.step(dis_optimizer)\n",
    "            LOSS_SCALER.update()\n",
    "            sum_dis_loss += float(dis_loss) * len(X)\n",
    "        n_iter += 1\n",
    "    avg_gen_loss = sum_gen_loss * N_DIS / train_size\n",
    "    avg_dis_loss = sum_dis_loss / train_size\n",
    "    loss_viz.add_value('G loss', avg_gen_loss) # 訓練データに対する損失関数の値を記録\n",
    "    loss_viz.add_value('D loss', avg_dis_loss) # 同上\n",
    "    print('generator train loss = {0:.6f}'.format(avg_gen_loss))\n",
    "    print('discriminator train loss = {0:.6f}'.format(avg_dis_loss))\n",
    "    print('')\n",
    "\n",
    "    # 検証（学習経過の表示，モデル自動保存）\n",
    "    if epoch == 0 or (epoch + 1) % INTERVAL_FOR_SHOWING_PROGRESS == 0:\n",
    "        gen_model.eval()\n",
    "        dis_model.eval()\n",
    "        if epoch == 0:\n",
    "            real = to_sigmoid_image(real) # to_sigmoid_image 関数を用い，画素値が 0〜1 の範囲となるように調整する\n",
    "            show_images(real.to('cpu').detach(), num=32, num_per_row=8, title='real images', save_fig=False, save_dir=MODEL_DIR)\n",
    "        with torch.inference_mode():\n",
    "            fake = gen_model(Z_valid, L_valid) # 事前に用意しておいた検証用乱数と属性ラベル情報からFake画像を生成\n",
    "            #fake = gen_model(torch.randn(BATCH_SIZE, N).to(DEVICE), torch.zeros(BATCH_SIZE, len(TARGET_ATTRIBUTES)).to(DEVICE)) # エポックごとに異なる乱数を使用する場合はこのようにする\n",
    "        fake = to_sigmoid_image(fake) # to_sigmoid_image 関数を用い，画素値が 0〜1 の範囲となるように調整する\n",
    "        show_images(fake.to('cpu').detach(), num=32, num_per_row=8, title='epoch {0}'.format(epoch + 1), save_fig=False, save_dir=MODEL_DIR)\n",
    "        torch.save(gen_model.state_dict(), autosaved_model_name(MODEL_FILE_G, epoch + 1)) # 学習途中のモデルを保存したい場合はこのようにする\n",
    "\n",
    "    # 現在の学習状態を一時ファイル（チェックポイント）に保存\n",
    "    save_checkpoint(CHECKPOINT_EPOCH, CHECKPOINT_GEN_MODEL, CHECKPOINT_GEN_OPT, epoch+1, gen_model, gen_optimizer)\n",
    "    save_checkpoint(CHECKPOINT_EPOCH, CHECKPOINT_DIS_MODEL, CHECKPOINT_DIS_OPT, epoch+1, dis_model, dis_optimizer)\n",
    "\n",
    "# 学習結果のニューラルネットワークモデルをファイルに保存\n",
    "gen_model = gen_model.to('cpu')\n",
    "dis_model = dis_model.to('cpu')\n",
    "torch.save(gen_model.state_dict(), MODEL_FILE_G)\n",
    "#torch.save(dis_model.state_dict(), MODEL_FILE_D) # ディスクリミネータも保存したい場合はこのようにする\n",
    "\n",
    "# 損失関数の記録をファイルに保存\n",
    "loss_viz.save(v_file=os.path.join(MODEL_DIR, 'loss_graph.png'), h_file=os.path.join(MODEL_DIR, 'loss_history.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o30Rmkph0lMs"
   },
   "source": [
    "##### 必要なら，現在の学習経過情報を自分のgoogle driveに退避\n",
    "- Google Colab以外の環境で使用することは想定していません．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RjUdDI0X0tmr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "\n",
    "# 退避先フォルダの名称\n",
    "# 自分のgoogle driveにおけるルートフォルダの直下にこの名前のフォルダが作成される\n",
    "# 既に存在する場合は一度削除した上で再作成される（削除したフォルダはgoogle driveのゴミ箱に移動するようです．必要に応じて完全に削除して下さい）\n",
    "DST_DIR_NAME = 'AI_advanced_temp'\n",
    "\n",
    "# tempフォルダをまるごとgoogle driveに退避\n",
    "MOUNT_POINT = '/content/drive'\n",
    "DST_DIR_PATH = os.path.join(MOUNT_POINT, 'MyDrive', DST_DIR_NAME)\n",
    "if not os.path.isdir(MOUNT_POINT):\n",
    "    drive.mount(MOUNT_POINT)\n",
    "if os.path.exists(DST_DIR_PATH):\n",
    "    !rm -r $DST_DIR_PATH\n",
    "!cp -r temp $DST_DIR_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWDyWUh1X6vD"
   },
   "source": [
    "##### 学習済みニューラルネットワークモデルのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kz3qvcZPX6vD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# ニューラルネットワークモデルとその学習済みパラメータをファイルからロード\n",
    "gen_model = Generator(C=C, H=H, W=W, N=N, K=len(TARGET_ATTRIBUTES))\n",
    "gen_model.load_state_dict(torch.load(MODEL_FILE_G, weights_only=True))\n",
    "#gen_model.load_state_dict(torch.load(autosaved_model_name(MODEL_FILE_G, 90), weights_only=True)) # 例えば90エポック目のモデルをロードしたい場合は，このようにする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qCG2UvQX6vE"
   },
   "source": [
    "##### テスト処理1\n",
    "- 正規分布に従って複数の乱数ベクトルをランダムサンプリングし，それをデコーダに通して画像を生成．属性ラベルは固定値で指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2gCcoJaX6vE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from mylib.data_io import show_images, to_sigmoid_image\n",
    "\n",
    "\n",
    "gen_model = gen_model.to(DEVICE)\n",
    "gen_model.eval()\n",
    "\n",
    "# 生成する画像の枚数\n",
    "n_gen = 32\n",
    "\n",
    "# 属性ラベルの指定値\n",
    "# このサンプルコードでは TARGET_ATTRIBUTES = ['Blond_Hair', 'Brown_Hair', 'Black_Hair', 'Gray_Hair', 'Eyeglasses', 'Male', 'Young'] と設定しているので，\n",
    "#   'Blond_Hair' = 0, # ブロンド髪ではない\n",
    "#   'Brown_Hair' = 0, # 茶髪ではない\n",
    "#   'Black_Hair' = 1, # 黒髪である\n",
    "#   'Gray_Hair'  = 0, # 白髪ではない\n",
    "#   'Eyeglasses' = 0, # 眼鏡やサングラスをかけていない\n",
    "#   'Male'       = 0, # 男性でない（== 女性）\n",
    "#   'Young'      = 1, # 若い\n",
    "# という意味になる\n",
    "attributes = [0, 0, 1, 0, 0, 0, 1]\n",
    "\n",
    "# 標準正規分布 N(0, 1) に従って適当に乱数ベクトルを作成\n",
    "Z = torch.randn((n_gen, N)).to(DEVICE)\n",
    "\n",
    "# 属性ラベル情報の作成\n",
    "L = torch.tensor([attributes], dtype=torch.float32).repeat((n_gen, 1)).to(DEVICE)\n",
    "\n",
    "# 乱数ベクトルと属性ラベルをデコーダに入力し，その結果を表示\n",
    "with torch.inference_mode():\n",
    "    Y = gen_model(Z, L)\n",
    "    Y = to_sigmoid_image(Y)\n",
    "    show_images(Y.to('cpu').detach(), num=n_gen, num_per_row=8, title='CGAN_sample_generated_case1', save_fig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ajlzp6GSX6vE"
   },
   "source": [
    "##### テスト処理2\n",
    "- 乱数ベクトルを一つだけサンプリングし，それをデコーダに通して画像を生成．属性ラベルは，一つの次元を徐々に変化させる形で指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvJIp9r2X6vE"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from mylib.data_io import show_images, to_sigmoid_image\n",
    "\n",
    "\n",
    "gen_model = gen_model.to(DEVICE)\n",
    "gen_model.eval()\n",
    "\n",
    "# ベースとなる属性ラベル\n",
    "base_attributes = [0, 0, 1, 0, 0, 0, 1]\n",
    "\n",
    "# 上の属性ラベルのうち何番目の属性値を変化させるか\n",
    "# 以下の例は\n",
    "#   - 0番目の属性（ 'Blond_Hair', ベース値 0 ）を 0 から 1 に徐々に変化\n",
    "#   - 2番目の属性（ 'Black_Hair', ベース値 1 ）を 1 から 0 に徐々に変化\n",
    "# という意味になり，すなわち，ブロンド髪から黒髪への属性変化に相当\n",
    "targets = [0, 2]\n",
    "\n",
    "# 生成する画像の枚数\n",
    "n_gen = 16 # 上で指定した属性ラベルを 0～1 の間で n_gen 段階に変化させる\n",
    "\n",
    "# 標準正規分布 N(0, 1) に従って適当に乱数ベクトルを作成\n",
    "Z = torch.randn((1, N)).repeat((n_gen, 1)).to(DEVICE)\n",
    "\n",
    "# 属性ラベル情報の作成\n",
    "L = []\n",
    "for i in range(n_gen):\n",
    "    attributes = copy.deepcopy(base_attributes)\n",
    "    for t in targets:\n",
    "        # t番目の属性値を 0〜1 の範囲でずらす\n",
    "        if base_attributes[t] == 0:\n",
    "            attributes[t] = i / (n_gen - 1)\n",
    "        else:\n",
    "            attributes[t] = 1 - i / (n_gen - 1)\n",
    "    L.append(attributes)\n",
    "L = torch.tensor(L, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "# 乱数ベクトルと属性ラベルをデコーダに入力し，その結果を表示\n",
    "with torch.inference_mode():\n",
    "    Y = gen_model(Z, L)\n",
    "    Y = to_sigmoid_image(Y)\n",
    "    show_images(Y.to('cpu').detach(), num=n_gen, num_per_row=8, title='CGAN_sample_generated_case2', save_fig=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
